{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":84909,"databundleVersionId":9660770,"sourceType":"competition"}],"dockerImageVersionId":30761,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import f1_score\nimport torch.nn.functional as F\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-06T14:56:19.820507Z","iopub.execute_input":"2024-10-06T14:56:19.821331Z","iopub.status.idle":"2024-10-06T14:56:19.833396Z","shell.execute_reply.started":"2024-10-06T14:56:19.821263Z","shell.execute_reply":"2024-10-06T14:56:19.832125Z"},"trusted":true},"execution_count":146,"outputs":[{"name":"stdout","text":"/kaggle/input/2024-artificial-intelligence-hw-1/sample_submission.csv\n/kaggle/input/2024-artificial-intelligence-hw-1/train.csv\n/kaggle/input/2024-artificial-intelligence-hw-1/test.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Define Linear Classifier","metadata":{}},{"cell_type":"code","source":"class LinearClassifier(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super(LinearClassifier, self).__init__()\n        self.fc1 = nn.Linear(input_dim, 128)  # Increase number of neurons\n        self.fc2 = nn.Linear(128, 64)  # Add an additional hidden layer\n        self.fc3 = nn.Linear(64, output_dim)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))  # Additional layer with activation\n        output = self.fc3(x)\n        return output\n","metadata":{"execution":{"iopub.status.busy":"2024-10-06T14:56:19.835787Z","iopub.execute_input":"2024-10-06T14:56:19.837158Z","iopub.status.idle":"2024-10-06T14:56:19.846598Z","shell.execute_reply.started":"2024-10-06T14:56:19.837097Z","shell.execute_reply":"2024-10-06T14:56:19.845445Z"},"trusted":true},"execution_count":147,"outputs":[]},{"cell_type":"markdown","source":"## Load Data","metadata":{}},{"cell_type":"code","source":"# load train.csv\ntrain_df = pd.read_csv(\"/kaggle/input/2024-artificial-intelligence-hw-1/train.csv\")\ntrain_df","metadata":{"execution":{"iopub.status.busy":"2024-10-06T14:56:19.871841Z","iopub.execute_input":"2024-10-06T14:56:19.872296Z","iopub.status.idle":"2024-10-06T14:56:19.976930Z","shell.execute_reply.started":"2024-10-06T14:56:19.872253Z","shell.execute_reply":"2024-10-06T14:56:19.975514Z"},"trusted":true},"execution_count":148,"outputs":[{"execution_count":148,"output_type":"execute_result","data":{"text/plain":"      date   close    open    high     low    volume  ht_dcperiod  ht_dcphase  \\\n0        1   94.86  101.36  103.36   94.36  21789208    16.827667  307.142904   \n1        2   97.59   96.09   99.09   93.09  13642091    15.896730  -22.568827   \n2        3   97.59   94.09  101.59   92.09  16305307    15.562605   -7.225711   \n3        4   98.69  100.19  102.69   98.69  14127338    15.367623    6.165014   \n4        5   96.50  102.00  103.50   95.50  10304396    15.549624   16.710140   \n...    ...     ...     ...     ...     ...       ...          ...         ...   \n1857  1858  131.64  131.64  133.14  131.64   1915360    21.933646  254.843013   \n1858  1859  133.34  132.34  134.84  132.34   2765566    20.412209  305.502984   \n1859  1860  134.19  133.19  134.69  132.69   1102925    20.218289  -37.385040   \n1860  1861  133.77  134.27  136.27  133.27   1224191    20.742574  -29.806261   \n1861  1862  132.91  133.91  134.41  132.41   1209156    21.267868  -15.366448   \n\n       inphase  quadrature  ...        var       atr      natr  trange  \\\n0    -3.555735   -3.246850  ...  10.243520  7.920250  8.349410     9.0   \n1    -2.639682    1.304832  ...   7.281680  7.821542  8.014696     6.0   \n2    -3.147920   -1.168940  ...   6.959584  7.941432  8.137547     9.5   \n3    -3.464886   -0.187496  ...   3.781264  7.738472  7.841192     5.1   \n4    -3.691806   -1.918849  ...   3.172496  7.757153  8.038501     8.0   \n...        ...         ...  ...        ...       ...       ...     ...   \n1857  1.760688   -0.225027  ...   4.348616  2.505854  1.903565     1.5   \n1858 -0.752763   -6.230444  ...   0.751400  2.571374  1.928434     2.5   \n1859 -3.042672   -4.300574  ...   0.751400  2.530561  1.885805     2.0   \n1860 -3.829639   -1.516603  ...   0.839464  2.564093  1.916792     3.0   \n1861 -4.013364    2.116116  ...   0.362960  2.523800  1.898879     2.0   \n\n                ad         adosc          obv  1_trend  5_trend  10_trend  \n0    -3.848627e+07 -1.167386e+07  261516573.0      1.0      1.0       1.0  \n1    -4.412492e+07 -1.520909e+07  258865213.0      0.0      1.0       1.0  \n2    -4.155040e+07 -1.235346e+07  258865213.0      1.0      1.0       1.0  \n3    -5.567774e+07 -1.455727e+07  272992551.0     -1.0      0.0       1.0  \n4    -6.340604e+07 -1.659444e+07  262688155.0      1.0      1.0       1.0  \n...            ...           ...          ...      ...      ...       ...  \n1857 -1.378997e+09 -1.715582e+06  853638722.0      1.0     -1.0      -1.0  \n1858 -1.378417e+09 -1.447268e+06  859048286.0      0.0     -1.0      -1.0  \n1859 -1.377866e+09 -1.119953e+06  860151211.0      0.0     -1.0      -1.0  \n1860 -1.378682e+09 -1.143915e+06  858927020.0      0.0     -1.0      -1.0  \n1861 -1.379287e+09 -1.242091e+06  857717864.0     -1.0     -1.0      -1.0  \n\n[1862 rows x 93 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>close</th>\n      <th>open</th>\n      <th>high</th>\n      <th>low</th>\n      <th>volume</th>\n      <th>ht_dcperiod</th>\n      <th>ht_dcphase</th>\n      <th>inphase</th>\n      <th>quadrature</th>\n      <th>...</th>\n      <th>var</th>\n      <th>atr</th>\n      <th>natr</th>\n      <th>trange</th>\n      <th>ad</th>\n      <th>adosc</th>\n      <th>obv</th>\n      <th>1_trend</th>\n      <th>5_trend</th>\n      <th>10_trend</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>94.86</td>\n      <td>101.36</td>\n      <td>103.36</td>\n      <td>94.36</td>\n      <td>21789208</td>\n      <td>16.827667</td>\n      <td>307.142904</td>\n      <td>-3.555735</td>\n      <td>-3.246850</td>\n      <td>...</td>\n      <td>10.243520</td>\n      <td>7.920250</td>\n      <td>8.349410</td>\n      <td>9.0</td>\n      <td>-3.848627e+07</td>\n      <td>-1.167386e+07</td>\n      <td>261516573.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>97.59</td>\n      <td>96.09</td>\n      <td>99.09</td>\n      <td>93.09</td>\n      <td>13642091</td>\n      <td>15.896730</td>\n      <td>-22.568827</td>\n      <td>-2.639682</td>\n      <td>1.304832</td>\n      <td>...</td>\n      <td>7.281680</td>\n      <td>7.821542</td>\n      <td>8.014696</td>\n      <td>6.0</td>\n      <td>-4.412492e+07</td>\n      <td>-1.520909e+07</td>\n      <td>258865213.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>97.59</td>\n      <td>94.09</td>\n      <td>101.59</td>\n      <td>92.09</td>\n      <td>16305307</td>\n      <td>15.562605</td>\n      <td>-7.225711</td>\n      <td>-3.147920</td>\n      <td>-1.168940</td>\n      <td>...</td>\n      <td>6.959584</td>\n      <td>7.941432</td>\n      <td>8.137547</td>\n      <td>9.5</td>\n      <td>-4.155040e+07</td>\n      <td>-1.235346e+07</td>\n      <td>258865213.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>98.69</td>\n      <td>100.19</td>\n      <td>102.69</td>\n      <td>98.69</td>\n      <td>14127338</td>\n      <td>15.367623</td>\n      <td>6.165014</td>\n      <td>-3.464886</td>\n      <td>-0.187496</td>\n      <td>...</td>\n      <td>3.781264</td>\n      <td>7.738472</td>\n      <td>7.841192</td>\n      <td>5.1</td>\n      <td>-5.567774e+07</td>\n      <td>-1.455727e+07</td>\n      <td>272992551.0</td>\n      <td>-1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>96.50</td>\n      <td>102.00</td>\n      <td>103.50</td>\n      <td>95.50</td>\n      <td>10304396</td>\n      <td>15.549624</td>\n      <td>16.710140</td>\n      <td>-3.691806</td>\n      <td>-1.918849</td>\n      <td>...</td>\n      <td>3.172496</td>\n      <td>7.757153</td>\n      <td>8.038501</td>\n      <td>8.0</td>\n      <td>-6.340604e+07</td>\n      <td>-1.659444e+07</td>\n      <td>262688155.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1857</th>\n      <td>1858</td>\n      <td>131.64</td>\n      <td>131.64</td>\n      <td>133.14</td>\n      <td>131.64</td>\n      <td>1915360</td>\n      <td>21.933646</td>\n      <td>254.843013</td>\n      <td>1.760688</td>\n      <td>-0.225027</td>\n      <td>...</td>\n      <td>4.348616</td>\n      <td>2.505854</td>\n      <td>1.903565</td>\n      <td>1.5</td>\n      <td>-1.378997e+09</td>\n      <td>-1.715582e+06</td>\n      <td>853638722.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>1858</th>\n      <td>1859</td>\n      <td>133.34</td>\n      <td>132.34</td>\n      <td>134.84</td>\n      <td>132.34</td>\n      <td>2765566</td>\n      <td>20.412209</td>\n      <td>305.502984</td>\n      <td>-0.752763</td>\n      <td>-6.230444</td>\n      <td>...</td>\n      <td>0.751400</td>\n      <td>2.571374</td>\n      <td>1.928434</td>\n      <td>2.5</td>\n      <td>-1.378417e+09</td>\n      <td>-1.447268e+06</td>\n      <td>859048286.0</td>\n      <td>0.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>1859</th>\n      <td>1860</td>\n      <td>134.19</td>\n      <td>133.19</td>\n      <td>134.69</td>\n      <td>132.69</td>\n      <td>1102925</td>\n      <td>20.218289</td>\n      <td>-37.385040</td>\n      <td>-3.042672</td>\n      <td>-4.300574</td>\n      <td>...</td>\n      <td>0.751400</td>\n      <td>2.530561</td>\n      <td>1.885805</td>\n      <td>2.0</td>\n      <td>-1.377866e+09</td>\n      <td>-1.119953e+06</td>\n      <td>860151211.0</td>\n      <td>0.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>1860</th>\n      <td>1861</td>\n      <td>133.77</td>\n      <td>134.27</td>\n      <td>136.27</td>\n      <td>133.27</td>\n      <td>1224191</td>\n      <td>20.742574</td>\n      <td>-29.806261</td>\n      <td>-3.829639</td>\n      <td>-1.516603</td>\n      <td>...</td>\n      <td>0.839464</td>\n      <td>2.564093</td>\n      <td>1.916792</td>\n      <td>3.0</td>\n      <td>-1.378682e+09</td>\n      <td>-1.143915e+06</td>\n      <td>858927020.0</td>\n      <td>0.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>1861</th>\n      <td>1862</td>\n      <td>132.91</td>\n      <td>133.91</td>\n      <td>134.41</td>\n      <td>132.41</td>\n      <td>1209156</td>\n      <td>21.267868</td>\n      <td>-15.366448</td>\n      <td>-4.013364</td>\n      <td>2.116116</td>\n      <td>...</td>\n      <td>0.362960</td>\n      <td>2.523800</td>\n      <td>1.898879</td>\n      <td>2.0</td>\n      <td>-1.379287e+09</td>\n      <td>-1.242091e+06</td>\n      <td>857717864.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>1862 rows × 93 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Preprocess","metadata":{}},{"cell_type":"code","source":"# CrossEntropyLoss need values start from 0 and can't use negative value\nclass_transform = {-1:0, 0:1, 1:2}\n\n# Create feature. Per 30 days data as an example.\ndef create_lagged_features(df, days=30):\n    features = []\n    targets_1d = []\n    targets_5d = []\n    targets_10d = []\n    \n    for i in range(days-1, len(df)):\n        feature_row = []\n        target_1d = class_transform[df.iloc[i]['1_trend']]\n        target_5d = class_transform[df.iloc[i]['5_trend']]\n        target_10d = class_transform[df.iloc[i]['10_trend']]\n\n        # Concat features from day 1 to day 30.\n        for j in range(days-1, -1, -1):\n            feature_row.extend([\n                df.iloc[i - j]['close'],\n                df.iloc[i - j]['open'],\n                df.iloc[i - j]['high'],\n                df.iloc[i - j]['low'],\n                df.iloc[i - j]['volume']\n            ])\n        features.append(feature_row)\n        targets_1d.append(target_1d)\n        targets_5d.append(target_5d)\n        targets_10d.append(target_10d)\n    \n    return np.array(features), np.array(targets_1d), np.array(targets_5d), np.array(targets_10d)\n\n# Create features and targets for 1-day, 5-day, and 10-day trends\nfeatures, targets_1d, targets_5d, targets_10d = create_lagged_features(train_df, days=30)","metadata":{"execution":{"iopub.status.busy":"2024-10-06T14:56:19.979337Z","iopub.execute_input":"2024-10-06T14:56:19.979951Z","iopub.status.idle":"2024-10-06T14:57:04.393176Z","shell.execute_reply.started":"2024-10-06T14:56:19.979893Z","shell.execute_reply":"2024-10-06T14:57:04.392220Z"},"trusted":true},"execution_count":149,"outputs":[]},{"cell_type":"code","source":"print(features.shape)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-06T14:57:04.395240Z","iopub.execute_input":"2024-10-06T14:57:04.395646Z","iopub.status.idle":"2024-10-06T14:57:04.401142Z","shell.execute_reply.started":"2024-10-06T14:57:04.395606Z","shell.execute_reply":"2024-10-06T14:57:04.400024Z"},"trusted":true},"execution_count":150,"outputs":[{"name":"stdout","text":"(1833, 150)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Standardization & Split Data","metadata":{}},{"cell_type":"code","source":"# Define StandardScaler for Standardization\nscaler = StandardScaler()\n\nX_train, X_val = features[:round(len(features) * 0.8)], features[round(len(features) * 0.8):]\nX_train = torch.tensor(scaler.fit_transform(X_train), dtype=torch.float32)\nX_val = torch.tensor(scaler.transform(X_val), dtype=torch.float32)\n\ny_train_1d = torch.from_numpy((targets_1d[:round(len(targets_1d) * 0.8)]).squeeze()).long()\ny_val_1d = torch.from_numpy((targets_1d[round(len(targets_1d) * 0.8):]).squeeze()).long()\n\ny_train_5d = torch.from_numpy((targets_5d[:round(len(targets_5d) * 0.8)]).squeeze()).long()\ny_val_5d = torch.from_numpy((targets_5d[round(len(targets_5d) * 0.8):]).squeeze()).long()\n\ny_train_10d = torch.from_numpy((targets_10d[:round(len(targets_10d) * 0.8)]).squeeze()).long()\ny_val_10d = torch.from_numpy((targets_10d[round(len(targets_10d) * 0.8):]).squeeze()).long()\n\n# Move data to device (GPU or CPU)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nX_train, X_val = X_train.to(device), X_val.to(device)\ny_train_1d, y_val_1d = y_train_1d.to(device), y_val_1d.to(device)\ny_train_5d, y_val_5d = y_train_5d.to(device), y_val_5d.to(device)\ny_train_10d, y_val_10d = y_train_10d.to(device), y_val_10d.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-10-06T14:57:04.402823Z","iopub.execute_input":"2024-10-06T14:57:04.403235Z","iopub.status.idle":"2024-10-06T14:57:04.423806Z","shell.execute_reply.started":"2024-10-06T14:57:04.403197Z","shell.execute_reply":"2024-10-06T14:57:04.422611Z"},"trusted":true},"execution_count":151,"outputs":[]},{"cell_type":"code","source":"# Use DataLoader for each model's dataset\ndef get_dataloader(X, y):\n    dataset = TensorDataset(X, y)\n    return DataLoader(dataset, batch_size=1024)\n\ntrain_loader_1d = get_dataloader(X_train, y_train_1d)\nval_loader_1d = get_dataloader(X_val, y_val_1d)\n\ntrain_loader_5d = get_dataloader(X_train, y_train_5d)\nval_loader_5d = get_dataloader(X_val, y_val_5d)\n\ntrain_loader_10d = get_dataloader(X_train, y_train_10d)\nval_loader_10d = get_dataloader(X_val, y_val_10d)","metadata":{"execution":{"iopub.status.busy":"2024-10-06T14:57:04.426830Z","iopub.execute_input":"2024-10-06T14:57:04.427220Z","iopub.status.idle":"2024-10-06T14:57:04.439906Z","shell.execute_reply.started":"2024-10-06T14:57:04.427180Z","shell.execute_reply":"2024-10-06T14:57:04.438783Z"},"trusted":true},"execution_count":152,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"markdown","source":"### Define Validation Process","metadata":{}},{"cell_type":"code","source":"def validate_loss(model):\n    model.eval()\n    valid_pred = []\n    valid_y = []\n    for val_batch_X, val_batch_y in val_dataloader:\n        # Stop gradient calculation in inference process\n        with torch.no_grad():\n            val_outputs = model(val_batch_X)\n            val_loss = criterion(val_outputs, val_batch_y)\n    return val_loss.item()","metadata":{"execution":{"iopub.status.busy":"2024-10-06T14:57:04.441282Z","iopub.execute_input":"2024-10-06T14:57:04.441671Z","iopub.status.idle":"2024-10-06T14:57:04.451737Z","shell.execute_reply.started":"2024-10-06T14:57:04.441632Z","shell.execute_reply":"2024-10-06T14:57:04.450624Z"},"trusted":true},"execution_count":153,"outputs":[]},{"cell_type":"markdown","source":"### Start Training","metadata":{}},{"cell_type":"code","source":"input_size = 30 * 5  # Each sample has 30 days of 5 features (close, open, high, low, volume)\noutput_size = 3      # 3 classes for trend prediction\n\nmodel_1d = LinearClassifier(input_size, output_size).to(device)\nmodel_5d = LinearClassifier(input_size, output_size).to(device)\nmodel_10d = LinearClassifier(input_size, output_size).to(device)\n\n# Loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer_1d = torch.optim.SGD(model_1d.parameters(), lr=1e-4)\noptimizer_5d = torch.optim.SGD(model_5d.parameters(), lr=1e-4)\noptimizer_10d = torch.optim.SGD(model_10d.parameters(), lr=1e-4)\n\n\n\n# Validation function\ndef validate_loss(model, val_loader):\n    model.eval()\n    total_val_loss = 0\n    with torch.no_grad():\n        for val_batch_X, val_batch_y in val_loader:\n            val_outputs = model(val_batch_X)\n            val_loss = criterion(val_outputs, val_batch_y)\n            total_val_loss += val_loss.item()\n    return total_val_loss / len(val_loader)\n\n# Training function for each model\ndef train_model(model, optimizer, train_loader, val_loader, num_epochs=5000):\n    for epoch in range(num_epochs):\n        model.train()\n        for batch_X, batch_y in train_loader:\n            outputs = model(batch_X)\n            loss = criterion(outputs, batch_y)\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        \n        if (epoch + 1) % 100 == 0:\n            val_loss = validate_loss(model, val_loader)\n            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Val Loss: {val_loss:.4f}')\n\n# Train each model\ntrain_model(model_1d, optimizer_1d, train_loader_1d, val_loader_1d)\ntrain_model(model_5d, optimizer_5d, train_loader_5d, val_loader_5d)\ntrain_model(model_10d, optimizer_10d, train_loader_10d, val_loader_10d)\n\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-10-06T14:57:04.453242Z","iopub.execute_input":"2024-10-06T14:57:04.453736Z","iopub.status.idle":"2024-10-06T15:02:41.934546Z","shell.execute_reply.started":"2024-10-06T14:57:04.453693Z","shell.execute_reply":"2024-10-06T15:02:41.933349Z"},"trusted":true},"execution_count":154,"outputs":[{"name":"stdout","text":"Epoch [100/5000], Loss: 1.1011, Val Loss: 1.1041\nEpoch [200/5000], Loss: 1.1008, Val Loss: 1.1038\nEpoch [300/5000], Loss: 1.1005, Val Loss: 1.1035\nEpoch [400/5000], Loss: 1.1003, Val Loss: 1.1032\nEpoch [500/5000], Loss: 1.1000, Val Loss: 1.1029\nEpoch [600/5000], Loss: 1.0998, Val Loss: 1.1026\nEpoch [700/5000], Loss: 1.0996, Val Loss: 1.1023\nEpoch [800/5000], Loss: 1.0994, Val Loss: 1.1020\nEpoch [900/5000], Loss: 1.0993, Val Loss: 1.1018\nEpoch [1000/5000], Loss: 1.0991, Val Loss: 1.1015\nEpoch [1100/5000], Loss: 1.0990, Val Loss: 1.1013\nEpoch [1200/5000], Loss: 1.0988, Val Loss: 1.1010\nEpoch [1300/5000], Loss: 1.0987, Val Loss: 1.1008\nEpoch [1400/5000], Loss: 1.0986, Val Loss: 1.1006\nEpoch [1500/5000], Loss: 1.0985, Val Loss: 1.1004\nEpoch [1600/5000], Loss: 1.0984, Val Loss: 1.1001\nEpoch [1700/5000], Loss: 1.0983, Val Loss: 1.0999\nEpoch [1800/5000], Loss: 1.0982, Val Loss: 1.0997\nEpoch [1900/5000], Loss: 1.0981, Val Loss: 1.0995\nEpoch [2000/5000], Loss: 1.0980, Val Loss: 1.0993\nEpoch [2100/5000], Loss: 1.0980, Val Loss: 1.0991\nEpoch [2200/5000], Loss: 1.0979, Val Loss: 1.0989\nEpoch [2300/5000], Loss: 1.0978, Val Loss: 1.0988\nEpoch [2400/5000], Loss: 1.0977, Val Loss: 1.0986\nEpoch [2500/5000], Loss: 1.0977, Val Loss: 1.0984\nEpoch [2600/5000], Loss: 1.0976, Val Loss: 1.0982\nEpoch [2700/5000], Loss: 1.0975, Val Loss: 1.0981\nEpoch [2800/5000], Loss: 1.0975, Val Loss: 1.0979\nEpoch [2900/5000], Loss: 1.0974, Val Loss: 1.0977\nEpoch [3000/5000], Loss: 1.0974, Val Loss: 1.0976\nEpoch [3100/5000], Loss: 1.0973, Val Loss: 1.0974\nEpoch [3200/5000], Loss: 1.0973, Val Loss: 1.0972\nEpoch [3300/5000], Loss: 1.0972, Val Loss: 1.0971\nEpoch [3400/5000], Loss: 1.0971, Val Loss: 1.0969\nEpoch [3500/5000], Loss: 1.0971, Val Loss: 1.0968\nEpoch [3600/5000], Loss: 1.0970, Val Loss: 1.0967\nEpoch [3700/5000], Loss: 1.0970, Val Loss: 1.0965\nEpoch [3800/5000], Loss: 1.0969, Val Loss: 1.0964\nEpoch [3900/5000], Loss: 1.0969, Val Loss: 1.0962\nEpoch [4000/5000], Loss: 1.0968, Val Loss: 1.0961\nEpoch [4100/5000], Loss: 1.0968, Val Loss: 1.0960\nEpoch [4200/5000], Loss: 1.0967, Val Loss: 1.0959\nEpoch [4300/5000], Loss: 1.0967, Val Loss: 1.0957\nEpoch [4400/5000], Loss: 1.0967, Val Loss: 1.0956\nEpoch [4500/5000], Loss: 1.0966, Val Loss: 1.0955\nEpoch [4600/5000], Loss: 1.0966, Val Loss: 1.0954\nEpoch [4700/5000], Loss: 1.0965, Val Loss: 1.0952\nEpoch [4800/5000], Loss: 1.0965, Val Loss: 1.0951\nEpoch [4900/5000], Loss: 1.0964, Val Loss: 1.0950\nEpoch [5000/5000], Loss: 1.0964, Val Loss: 1.0949\nEpoch [100/5000], Loss: 1.0882, Val Loss: 1.0952\nEpoch [200/5000], Loss: 1.0862, Val Loss: 1.0947\nEpoch [300/5000], Loss: 1.0843, Val Loss: 1.0942\nEpoch [400/5000], Loss: 1.0825, Val Loss: 1.0938\nEpoch [500/5000], Loss: 1.0808, Val Loss: 1.0933\nEpoch [600/5000], Loss: 1.0792, Val Loss: 1.0929\nEpoch [700/5000], Loss: 1.0776, Val Loss: 1.0925\nEpoch [800/5000], Loss: 1.0761, Val Loss: 1.0921\nEpoch [900/5000], Loss: 1.0747, Val Loss: 1.0917\nEpoch [1000/5000], Loss: 1.0734, Val Loss: 1.0913\nEpoch [1100/5000], Loss: 1.0721, Val Loss: 1.0909\nEpoch [1200/5000], Loss: 1.0709, Val Loss: 1.0906\nEpoch [1300/5000], Loss: 1.0697, Val Loss: 1.0903\nEpoch [1400/5000], Loss: 1.0686, Val Loss: 1.0900\nEpoch [1500/5000], Loss: 1.0675, Val Loss: 1.0897\nEpoch [1600/5000], Loss: 1.0665, Val Loss: 1.0894\nEpoch [1700/5000], Loss: 1.0655, Val Loss: 1.0891\nEpoch [1800/5000], Loss: 1.0646, Val Loss: 1.0888\nEpoch [1900/5000], Loss: 1.0637, Val Loss: 1.0886\nEpoch [2000/5000], Loss: 1.0628, Val Loss: 1.0883\nEpoch [2100/5000], Loss: 1.0620, Val Loss: 1.0881\nEpoch [2200/5000], Loss: 1.0612, Val Loss: 1.0879\nEpoch [2300/5000], Loss: 1.0604, Val Loss: 1.0876\nEpoch [2400/5000], Loss: 1.0597, Val Loss: 1.0874\nEpoch [2500/5000], Loss: 1.0590, Val Loss: 1.0872\nEpoch [2600/5000], Loss: 1.0583, Val Loss: 1.0871\nEpoch [2700/5000], Loss: 1.0576, Val Loss: 1.0869\nEpoch [2800/5000], Loss: 1.0570, Val Loss: 1.0867\nEpoch [2900/5000], Loss: 1.0564, Val Loss: 1.0866\nEpoch [3000/5000], Loss: 1.0558, Val Loss: 1.0864\nEpoch [3100/5000], Loss: 1.0553, Val Loss: 1.0863\nEpoch [3200/5000], Loss: 1.0548, Val Loss: 1.0861\nEpoch [3300/5000], Loss: 1.0542, Val Loss: 1.0860\nEpoch [3400/5000], Loss: 1.0537, Val Loss: 1.0859\nEpoch [3500/5000], Loss: 1.0533, Val Loss: 1.0858\nEpoch [3600/5000], Loss: 1.0528, Val Loss: 1.0857\nEpoch [3700/5000], Loss: 1.0523, Val Loss: 1.0856\nEpoch [3800/5000], Loss: 1.0519, Val Loss: 1.0855\nEpoch [3900/5000], Loss: 1.0515, Val Loss: 1.0855\nEpoch [4000/5000], Loss: 1.0511, Val Loss: 1.0854\nEpoch [4100/5000], Loss: 1.0507, Val Loss: 1.0853\nEpoch [4200/5000], Loss: 1.0503, Val Loss: 1.0853\nEpoch [4300/5000], Loss: 1.0499, Val Loss: 1.0852\nEpoch [4400/5000], Loss: 1.0496, Val Loss: 1.0852\nEpoch [4500/5000], Loss: 1.0492, Val Loss: 1.0851\nEpoch [4600/5000], Loss: 1.0489, Val Loss: 1.0851\nEpoch [4700/5000], Loss: 1.0485, Val Loss: 1.0851\nEpoch [4800/5000], Loss: 1.0482, Val Loss: 1.0851\nEpoch [4900/5000], Loss: 1.0479, Val Loss: 1.0850\nEpoch [5000/5000], Loss: 1.0476, Val Loss: 1.0850\nEpoch [100/5000], Loss: 1.1118, Val Loss: 1.1067\nEpoch [200/5000], Loss: 1.1071, Val Loss: 1.1046\nEpoch [300/5000], Loss: 1.1027, Val Loss: 1.1026\nEpoch [400/5000], Loss: 1.0986, Val Loss: 1.1006\nEpoch [500/5000], Loss: 1.0946, Val Loss: 1.0987\nEpoch [600/5000], Loss: 1.0908, Val Loss: 1.0969\nEpoch [700/5000], Loss: 1.0873, Val Loss: 1.0951\nEpoch [800/5000], Loss: 1.0839, Val Loss: 1.0934\nEpoch [900/5000], Loss: 1.0806, Val Loss: 1.0917\nEpoch [1000/5000], Loss: 1.0776, Val Loss: 1.0901\nEpoch [1100/5000], Loss: 1.0746, Val Loss: 1.0885\nEpoch [1200/5000], Loss: 1.0718, Val Loss: 1.0870\nEpoch [1300/5000], Loss: 1.0691, Val Loss: 1.0855\nEpoch [1400/5000], Loss: 1.0666, Val Loss: 1.0841\nEpoch [1500/5000], Loss: 1.0641, Val Loss: 1.0827\nEpoch [1600/5000], Loss: 1.0618, Val Loss: 1.0813\nEpoch [1700/5000], Loss: 1.0595, Val Loss: 1.0800\nEpoch [1800/5000], Loss: 1.0574, Val Loss: 1.0787\nEpoch [1900/5000], Loss: 1.0553, Val Loss: 1.0775\nEpoch [2000/5000], Loss: 1.0534, Val Loss: 1.0763\nEpoch [2100/5000], Loss: 1.0515, Val Loss: 1.0751\nEpoch [2200/5000], Loss: 1.0496, Val Loss: 1.0740\nEpoch [2300/5000], Loss: 1.0479, Val Loss: 1.0729\nEpoch [2400/5000], Loss: 1.0462, Val Loss: 1.0718\nEpoch [2500/5000], Loss: 1.0446, Val Loss: 1.0708\nEpoch [2600/5000], Loss: 1.0431, Val Loss: 1.0698\nEpoch [2700/5000], Loss: 1.0416, Val Loss: 1.0688\nEpoch [2800/5000], Loss: 1.0402, Val Loss: 1.0678\nEpoch [2900/5000], Loss: 1.0388, Val Loss: 1.0669\nEpoch [3000/5000], Loss: 1.0375, Val Loss: 1.0660\nEpoch [3100/5000], Loss: 1.0363, Val Loss: 1.0652\nEpoch [3200/5000], Loss: 1.0351, Val Loss: 1.0644\nEpoch [3300/5000], Loss: 1.0339, Val Loss: 1.0635\nEpoch [3400/5000], Loss: 1.0327, Val Loss: 1.0628\nEpoch [3500/5000], Loss: 1.0317, Val Loss: 1.0620\nEpoch [3600/5000], Loss: 1.0306, Val Loss: 1.0613\nEpoch [3700/5000], Loss: 1.0296, Val Loss: 1.0606\nEpoch [3800/5000], Loss: 1.0286, Val Loss: 1.0599\nEpoch [3900/5000], Loss: 1.0276, Val Loss: 1.0592\nEpoch [4000/5000], Loss: 1.0267, Val Loss: 1.0586\nEpoch [4100/5000], Loss: 1.0258, Val Loss: 1.0580\nEpoch [4200/5000], Loss: 1.0249, Val Loss: 1.0574\nEpoch [4300/5000], Loss: 1.0241, Val Loss: 1.0568\nEpoch [4400/5000], Loss: 1.0232, Val Loss: 1.0562\nEpoch [4500/5000], Loss: 1.0224, Val Loss: 1.0557\nEpoch [4600/5000], Loss: 1.0217, Val Loss: 1.0552\nEpoch [4700/5000], Loss: 1.0209, Val Loss: 1.0547\nEpoch [4800/5000], Loss: 1.0202, Val Loss: 1.0542\nEpoch [4900/5000], Loss: 1.0195, Val Loss: 1.0537\nEpoch [5000/5000], Loss: 1.0188, Val Loss: 1.0533\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Testing","metadata":{}},{"cell_type":"code","source":"test_df = pd.read_csv(\"/kaggle/input/2024-artificial-intelligence-hw-1/test.csv\")\ntest_df","metadata":{"execution":{"iopub.status.busy":"2024-10-06T15:02:41.936289Z","iopub.execute_input":"2024-10-06T15:02:41.937053Z","iopub.status.idle":"2024-10-06T15:02:42.369775Z","shell.execute_reply.started":"2024-10-06T15:02:41.936997Z","shell.execute_reply":"2024-10-06T15:02:42.368680Z"},"trusted":true},"execution_count":155,"outputs":[{"execution_count":155,"output_type":"execute_result","data":{"text/plain":"        id  date   close    open    high     low   volume  ht_dcperiod  \\\n0        0     1  143.99  142.99  143.99  141.99  1369798    16.290696   \n1        0     2  144.42  144.42  144.92  141.92  1196222    16.459398   \n2        0     3  145.27  145.77  146.27  144.77  1221903    16.608715   \n3        0     4  152.94  145.44  153.44  144.94  7824953    16.888389   \n4        0     5  149.53  151.53  155.03  148.53  4450101    17.143087   \n...    ...   ...     ...     ...     ...     ...      ...          ...   \n12805  426    26  152.60  149.60  152.60  149.60  2019624    28.715604   \n12806  426    27  154.86  155.36  155.86  154.36  1234617    28.910077   \n12807  426    28  154.41  152.41  155.41  151.41  1368270    30.321032   \n12808  426    29  156.21  155.71  158.21  154.71  1809694    31.793577   \n12809  426    30  155.76  154.76  157.26  154.26  4580941    32.022602   \n\n       ht_dcphase   inphase  ...  linearreg_slope    stddev         tsf  \\\n0      212.093803  1.204988  ...         0.134000  0.877460  144.782857   \n1      200.424594  2.559340  ...         0.131319  0.808490  144.854176   \n2      205.594921  1.388990  ...         0.203341  0.808490  145.425055   \n3      178.143808 -0.724966  ...         0.363319  3.294294  147.689890   \n4      167.720081 -1.350238  ...         0.461604  3.207036  148.822747   \n...           ...       ...  ...              ...       ...         ...   \n12805  208.685202  1.991429  ...        -0.153802  2.108867  152.455055   \n12806  219.954123 -4.560006  ...        -0.194593  2.570230  152.544835   \n12807  221.379706 -8.170982  ...        -0.190066  1.908639  152.623077   \n12808  219.653663 -7.402138  ...        -0.136835  1.162986  153.195165   \n12809  222.242054 -1.654245  ...        -0.036879  0.661362  153.933407   \n\n             var       atr      natr  trange            ad         adosc  \\\n0       0.769936  3.528314  2.450388    2.00 -1.405847e+09  1.350707e+06   \n1       0.653656  3.490577  2.416963    3.00 -1.405050e+09  1.618916e+06   \n2       0.653656  3.373393  2.322154    1.85 -1.405457e+09  1.451868e+06   \n3      10.852376  3.615324  2.363884    8.50 -1.397613e+09  3.646685e+06   \n4      10.285080  3.821372  2.555589    6.50 -1.400694e+09  3.192448e+06   \n...          ...       ...       ...     ...           ...           ...   \n12805   4.447320  4.077149  2.671788    3.00 -1.396747e+09 -1.638342e+06   \n12806   6.606080  3.821623  2.467792    1.50 -1.395967e+09 -5.561426e+05   \n12807   3.642904  3.834364  2.483236    4.00 -1.395283e+09 -1.754569e+05   \n12808   1.352536  3.831910  2.453050    3.80 -1.395541e+09 -8.603005e+04   \n12809   0.437400  3.772488  2.421987    3.00 -1.395541e+09 -4.162542e+04   \n\n               obv  \n0      871331963.0  \n1      872528185.0  \n2      873750088.0  \n3      882514490.0  \n4      878064389.0  \n...            ...  \n12805  880974185.0  \n12806  882761400.0  \n12807  881393130.0  \n12808  883202824.0  \n12809  878621883.0  \n\n[12810 rows x 91 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>date</th>\n      <th>close</th>\n      <th>open</th>\n      <th>high</th>\n      <th>low</th>\n      <th>volume</th>\n      <th>ht_dcperiod</th>\n      <th>ht_dcphase</th>\n      <th>inphase</th>\n      <th>...</th>\n      <th>linearreg_slope</th>\n      <th>stddev</th>\n      <th>tsf</th>\n      <th>var</th>\n      <th>atr</th>\n      <th>natr</th>\n      <th>trange</th>\n      <th>ad</th>\n      <th>adosc</th>\n      <th>obv</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1</td>\n      <td>143.99</td>\n      <td>142.99</td>\n      <td>143.99</td>\n      <td>141.99</td>\n      <td>1369798</td>\n      <td>16.290696</td>\n      <td>212.093803</td>\n      <td>1.204988</td>\n      <td>...</td>\n      <td>0.134000</td>\n      <td>0.877460</td>\n      <td>144.782857</td>\n      <td>0.769936</td>\n      <td>3.528314</td>\n      <td>2.450388</td>\n      <td>2.00</td>\n      <td>-1.405847e+09</td>\n      <td>1.350707e+06</td>\n      <td>871331963.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>2</td>\n      <td>144.42</td>\n      <td>144.42</td>\n      <td>144.92</td>\n      <td>141.92</td>\n      <td>1196222</td>\n      <td>16.459398</td>\n      <td>200.424594</td>\n      <td>2.559340</td>\n      <td>...</td>\n      <td>0.131319</td>\n      <td>0.808490</td>\n      <td>144.854176</td>\n      <td>0.653656</td>\n      <td>3.490577</td>\n      <td>2.416963</td>\n      <td>3.00</td>\n      <td>-1.405050e+09</td>\n      <td>1.618916e+06</td>\n      <td>872528185.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>3</td>\n      <td>145.27</td>\n      <td>145.77</td>\n      <td>146.27</td>\n      <td>144.77</td>\n      <td>1221903</td>\n      <td>16.608715</td>\n      <td>205.594921</td>\n      <td>1.388990</td>\n      <td>...</td>\n      <td>0.203341</td>\n      <td>0.808490</td>\n      <td>145.425055</td>\n      <td>0.653656</td>\n      <td>3.373393</td>\n      <td>2.322154</td>\n      <td>1.85</td>\n      <td>-1.405457e+09</td>\n      <td>1.451868e+06</td>\n      <td>873750088.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>4</td>\n      <td>152.94</td>\n      <td>145.44</td>\n      <td>153.44</td>\n      <td>144.94</td>\n      <td>7824953</td>\n      <td>16.888389</td>\n      <td>178.143808</td>\n      <td>-0.724966</td>\n      <td>...</td>\n      <td>0.363319</td>\n      <td>3.294294</td>\n      <td>147.689890</td>\n      <td>10.852376</td>\n      <td>3.615324</td>\n      <td>2.363884</td>\n      <td>8.50</td>\n      <td>-1.397613e+09</td>\n      <td>3.646685e+06</td>\n      <td>882514490.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>5</td>\n      <td>149.53</td>\n      <td>151.53</td>\n      <td>155.03</td>\n      <td>148.53</td>\n      <td>4450101</td>\n      <td>17.143087</td>\n      <td>167.720081</td>\n      <td>-1.350238</td>\n      <td>...</td>\n      <td>0.461604</td>\n      <td>3.207036</td>\n      <td>148.822747</td>\n      <td>10.285080</td>\n      <td>3.821372</td>\n      <td>2.555589</td>\n      <td>6.50</td>\n      <td>-1.400694e+09</td>\n      <td>3.192448e+06</td>\n      <td>878064389.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>12805</th>\n      <td>426</td>\n      <td>26</td>\n      <td>152.60</td>\n      <td>149.60</td>\n      <td>152.60</td>\n      <td>149.60</td>\n      <td>2019624</td>\n      <td>28.715604</td>\n      <td>208.685202</td>\n      <td>1.991429</td>\n      <td>...</td>\n      <td>-0.153802</td>\n      <td>2.108867</td>\n      <td>152.455055</td>\n      <td>4.447320</td>\n      <td>4.077149</td>\n      <td>2.671788</td>\n      <td>3.00</td>\n      <td>-1.396747e+09</td>\n      <td>-1.638342e+06</td>\n      <td>880974185.0</td>\n    </tr>\n    <tr>\n      <th>12806</th>\n      <td>426</td>\n      <td>27</td>\n      <td>154.86</td>\n      <td>155.36</td>\n      <td>155.86</td>\n      <td>154.36</td>\n      <td>1234617</td>\n      <td>28.910077</td>\n      <td>219.954123</td>\n      <td>-4.560006</td>\n      <td>...</td>\n      <td>-0.194593</td>\n      <td>2.570230</td>\n      <td>152.544835</td>\n      <td>6.606080</td>\n      <td>3.821623</td>\n      <td>2.467792</td>\n      <td>1.50</td>\n      <td>-1.395967e+09</td>\n      <td>-5.561426e+05</td>\n      <td>882761400.0</td>\n    </tr>\n    <tr>\n      <th>12807</th>\n      <td>426</td>\n      <td>28</td>\n      <td>154.41</td>\n      <td>152.41</td>\n      <td>155.41</td>\n      <td>151.41</td>\n      <td>1368270</td>\n      <td>30.321032</td>\n      <td>221.379706</td>\n      <td>-8.170982</td>\n      <td>...</td>\n      <td>-0.190066</td>\n      <td>1.908639</td>\n      <td>152.623077</td>\n      <td>3.642904</td>\n      <td>3.834364</td>\n      <td>2.483236</td>\n      <td>4.00</td>\n      <td>-1.395283e+09</td>\n      <td>-1.754569e+05</td>\n      <td>881393130.0</td>\n    </tr>\n    <tr>\n      <th>12808</th>\n      <td>426</td>\n      <td>29</td>\n      <td>156.21</td>\n      <td>155.71</td>\n      <td>158.21</td>\n      <td>154.71</td>\n      <td>1809694</td>\n      <td>31.793577</td>\n      <td>219.653663</td>\n      <td>-7.402138</td>\n      <td>...</td>\n      <td>-0.136835</td>\n      <td>1.162986</td>\n      <td>153.195165</td>\n      <td>1.352536</td>\n      <td>3.831910</td>\n      <td>2.453050</td>\n      <td>3.80</td>\n      <td>-1.395541e+09</td>\n      <td>-8.603005e+04</td>\n      <td>883202824.0</td>\n    </tr>\n    <tr>\n      <th>12809</th>\n      <td>426</td>\n      <td>30</td>\n      <td>155.76</td>\n      <td>154.76</td>\n      <td>157.26</td>\n      <td>154.26</td>\n      <td>4580941</td>\n      <td>32.022602</td>\n      <td>222.242054</td>\n      <td>-1.654245</td>\n      <td>...</td>\n      <td>-0.036879</td>\n      <td>0.661362</td>\n      <td>153.933407</td>\n      <td>0.437400</td>\n      <td>3.772488</td>\n      <td>2.421987</td>\n      <td>3.00</td>\n      <td>-1.395541e+09</td>\n      <td>-4.162542e+04</td>\n      <td>878621883.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>12810 rows × 91 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Adapted create_test_features function\ndef create_test_features(df, days=30):\n    features = []\n    for i in range(days-1, len(df)+1, days):\n        feature_row = []\n        for j in range(days-1, -1, -1):\n            feature_row.extend([\n                df.iloc[i - j]['close'],\n                df.iloc[i - j]['open'],\n                df.iloc[i - j]['high'],\n                df.iloc[i - j]['low'],\n                df.iloc[i - j]['volume']\n            ])\n        features.append(feature_row)\n    \n    return np.array(features)\n\n# Create test features\ntest_features = create_test_features(test_df, days=30)\n\n# Convert to PyTorch tensors\ntest_features = torch.from_numpy(test_features).float()\n\n# Standardize the test features\ntest_features = torch.tensor(scaler.transform(test_features), dtype=torch.float32)\ntest_features = test_features.to(device)\n\n# Run test features through each model and get predictions\ndef predict_for_model(model, test_features):\n    with torch.no_grad():\n        logits = model(test_features)\n        probs = F.softmax(logits, dim=1)  # Convert logits to probabilities\n        predicted_class = torch.argmax(probs, dim=1)  # Choose class with the highest probability\n    return predicted_class.cpu()\n\n# Predictions for each trend\npredicted_1d_class = predict_for_model(model_1d, test_features)\npredicted_5d_class = predict_for_model(model_5d, test_features)\npredicted_10d_class = predict_for_model(model_10d, test_features)\n\n# Print predictions for each trend\nprint(f'Predicted 1-day class: {predicted_1d_class}')\nprint(f'Predicted 5-day class: {predicted_5d_class}')\nprint(f'Predicted 10-day class: {predicted_10d_class}')\n","metadata":{"execution":{"iopub.status.busy":"2024-10-06T15:02:42.371649Z","iopub.execute_input":"2024-10-06T15:02:42.372127Z","iopub.status.idle":"2024-10-06T15:02:52.475055Z","shell.execute_reply.started":"2024-10-06T15:02:42.372076Z","shell.execute_reply":"2024-10-06T15:02:52.473865Z"},"trusted":true},"execution_count":156,"outputs":[{"name":"stdout","text":"Predicted 1-day class: tensor([2, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2,\n        1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,\n        1, 1, 0, 0, 0, 1, 2, 2, 1, 0, 1, 0, 1, 1, 2, 1, 1, 0, 1, 0, 1, 1, 0, 1,\n        1, 0, 2, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 2, 1, 2, 2, 1, 2, 1, 1, 2,\n        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 2, 0, 1, 1, 0, 1, 0,\n        0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,\n        0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 2, 1, 0, 1, 0, 0, 0, 1,\n        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,\n        1, 1, 0, 1, 1, 1, 2, 1, 1, 0, 1, 0, 0, 1, 0, 2, 1, 1, 1, 0, 0, 0, 1, 1,\n        1, 0, 1, 1, 0, 2, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 2, 0, 2,\n        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 2, 0, 0, 2, 0, 2, 1, 1, 1, 0, 2,\n        0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 2, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 2, 1, 1,\n        0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 2, 1, 1, 1, 0, 2, 2, 1, 0, 1, 0, 1, 0,\n        0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 2, 0, 1, 2, 1, 1, 0, 1, 1, 1, 2, 0, 0, 1,\n        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 2, 0, 1, 1, 0, 1,\n        0, 1, 2, 1, 1, 1, 1, 1, 2, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,\n        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 2, 1, 1, 1, 1, 0, 0, 1, 0, 0,\n        0, 2, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 2, 0, 0, 1, 2])\nPredicted 5-day class: tensor([2, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 2, 0, 2, 0, 2, 0, 2, 2, 0, 0, 0,\n        2, 2, 0, 2, 0, 0, 2, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 2,\n        0, 2, 0, 0, 0, 0, 2, 2, 2, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0,\n        0, 0, 2, 0, 0, 2, 0, 0, 0, 2, 0, 0, 2, 2, 2, 0, 0, 0, 2, 2, 2, 2, 2, 2,\n        2, 0, 0, 0, 2, 0, 0, 0, 2, 0, 2, 2, 2, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 0,\n        0, 2, 0, 2, 0, 2, 0, 2, 0, 0, 0, 0, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 2, 2, 0, 0, 0, 0, 0, 2,\n        0, 0, 2, 0, 0, 0, 2, 0, 2, 2, 2, 0, 2, 0, 0, 2, 2, 0, 2, 2, 0, 0, 0, 0,\n        0, 0, 0, 0, 2, 0, 0, 2, 0, 2, 2, 0, 0, 0, 0, 2, 2, 0, 2, 0, 0, 0, 0, 2,\n        0, 0, 0, 0, 0, 2, 0, 2, 2, 0, 0, 0, 2, 2, 2, 0, 0, 2, 2, 0, 0, 2, 2, 2,\n        0, 0, 0, 0, 2, 2, 2, 0, 2, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 2, 2, 0, 0, 2,\n        0, 2, 0, 2, 0, 0, 2, 0, 2, 2, 2, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 2, 0, 2,\n        0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 0, 0, 0, 2, 2, 0, 0, 2, 0, 0, 0, 0, 0,\n        0, 0, 0, 2, 2, 2, 0, 2, 2, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0,\n        0, 2, 0, 2, 0, 0, 2, 0, 0, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 0, 0, 0, 2,\n        0, 2, 2, 0, 0, 0, 0, 2, 2, 2, 0, 0, 0, 0, 2, 0, 2, 2, 0, 0, 0, 0, 0, 0,\n        0, 0, 2, 0, 2, 2, 2, 0, 0, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0, 2, 0, 2,\n        0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 2, 2, 0, 2, 2, 2])\nPredicted 10-day class: tensor([2, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 2, 0, 2, 0, 0, 0, 2, 2, 0, 2, 2,\n        2, 2, 0, 2, 0, 0, 2, 2, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0, 2,\n        0, 2, 0, 0, 0, 0, 2, 2, 2, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0,\n        0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 2, 2, 0, 0, 0, 2, 2, 2, 2, 2, 2,\n        2, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 2, 2, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 0,\n        0, 2, 0, 2, 0, 2, 0, 2, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 2, 2, 0, 0, 0, 0, 0, 2,\n        0, 0, 2, 0, 0, 0, 2, 0, 2, 2, 2, 0, 2, 0, 0, 2, 2, 0, 2, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 2, 0, 2, 2, 0, 2, 2, 0, 0, 0, 0, 2, 2, 0, 2, 2, 0, 0, 0, 2,\n        0, 2, 0, 0, 0, 2, 0, 2, 2, 0, 0, 0, 2, 2, 0, 0, 0, 2, 2, 0, 0, 2, 2, 2,\n        0, 0, 0, 0, 2, 2, 2, 0, 2, 0, 0, 0, 0, 2, 0, 0, 2, 0, 2, 2, 2, 0, 0, 2,\n        0, 2, 0, 2, 0, 0, 0, 0, 2, 2, 2, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 2, 0, 2,\n        0, 0, 2, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 2, 2, 0, 0, 2, 0, 2, 0, 0, 0,\n        0, 2, 0, 2, 2, 2, 0, 2, 2, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0,\n        0, 2, 0, 2, 0, 0, 2, 0, 0, 2, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 0, 0, 0, 2,\n        0, 2, 2, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 2, 0, 2, 2, 0, 2, 0, 0, 0, 0,\n        0, 0, 2, 0, 2, 0, 2, 0, 0, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 2, 0, 2,\n        0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 2, 2, 0, 2, 2, 0])\n","output_type":"stream"}]},{"cell_type":"code","source":"combined_predictions = np.concatenate([predicted_1d_class, predicted_5d_class, predicted_10d_class])\n\nids = np.arange(0, len(combined_predictions))\n\nresults_df = pd.DataFrame({\n    'id': ids,\n    'trend': combined_predictions\n})\nresults_df['trend'] = results_df['trend'].replace({2: 1, 1: 0, 0: -1})\nresults_df","metadata":{"execution":{"iopub.status.busy":"2024-10-06T15:02:52.476519Z","iopub.execute_input":"2024-10-06T15:02:52.476921Z","iopub.status.idle":"2024-10-06T15:02:52.494508Z","shell.execute_reply.started":"2024-10-06T15:02:52.476881Z","shell.execute_reply":"2024-10-06T15:02:52.493081Z"},"trusted":true},"execution_count":157,"outputs":[{"execution_count":157,"output_type":"execute_result","data":{"text/plain":"        id  trend\n0        0      1\n1        1     -1\n2        2     -1\n3        3      0\n4        4      0\n...    ...    ...\n1276  1276      1\n1277  1277     -1\n1278  1278      1\n1279  1279      1\n1280  1280     -1\n\n[1281 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>trend</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1276</th>\n      <td>1276</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1277</th>\n      <td>1277</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>1278</th>\n      <td>1278</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1279</th>\n      <td>1279</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1280</th>\n      <td>1280</td>\n      <td>-1</td>\n    </tr>\n  </tbody>\n</table>\n<p>1281 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"results_df.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-10-06T15:02:52.498626Z","iopub.execute_input":"2024-10-06T15:02:52.499173Z","iopub.status.idle":"2024-10-06T15:02:52.508609Z","shell.execute_reply.started":"2024-10-06T15:02:52.499119Z","shell.execute_reply":"2024-10-06T15:02:52.507422Z"},"trusted":true},"execution_count":158,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}